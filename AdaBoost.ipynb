{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Adaboost_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzO4ym2YyRZU",
        "colab_type": "text"
      },
      "source": [
        "# Importações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kjizKmLyioY",
        "colab_type": "code",
        "outputId": "c3c47e61-6fc9-457e-e01e-ff55f05ac4d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtH3drQYyRZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from statistics import mean "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zjoFSwAyRZi",
        "colab_type": "text"
      },
      "source": [
        "# Leitura de dados e labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFaU1NIpyRZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/Adaboost/sonar.all-data.csv', header=None)\n",
        "\n",
        "sonar_df = pd.DataFrame(data)\n",
        "labels_df = pd.DataFrame()\n",
        "\n",
        "# Muda a ordem das linhas do dataframe de forma aleatória\n",
        "sonar_df = sonar_df.sample(frac=1)\n",
        "\n",
        "# Valores do dataset\n",
        "data = sonar_df.iloc[:, :-1].values\n",
        "# Classes\n",
        "labels = sonar_df.iloc[:, 60].values\n",
        "\n",
        "# Mudando os labels: R -> -1 e M -> 1\n",
        "for y in range(len(labels)):\n",
        "    if labels[y] == 'R':\n",
        "        labels[y] = -1.0\n",
        "    else:\n",
        "        labels[y] = 1.0\n",
        "        \n",
        "labels = labels.astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr4w7hHdyRZo",
        "colab_type": "text"
      },
      "source": [
        "# Normalizando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAGFTyB0yRZp",
        "colab_type": "code",
        "outputId": "e4037acb-41d5-4769-982a-0ab816272eab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler() \n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "print(data)\n",
        "\n",
        "np.min(data[:,0]), np.max(data[:,0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.08112094 0.06258037 0.06668857 ... 0.08695652 0.23415978 0.12009238]\n",
            " [0.13864307 0.04929276 0.11990802 ... 0.07551487 0.03305785 0.02309469]\n",
            " [0.18215339 0.24689241 0.35611038 ... 0.36842105 0.25895317 0.16628176]\n",
            " ...\n",
            " [0.02581121 0.00471496 0.08377135 ... 0.13729977 0.04407713 0.05080831]\n",
            " [0.30457227 0.23531933 0.22766097 ... 0.18535469 0.30853994 0.09930716]\n",
            " [0.47861357 0.24389198 0.271682   ... 0.36384439 0.29752066 0.16859122]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "488H4pk1Cikb",
        "colab_type": "text"
      },
      "source": [
        "# Sub-amostragem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p5YrffJCgJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sub_amostragem (tam, X, y, D):\n",
        "    \n",
        "    X_sub = pd.DataFrame(X) \n",
        "    y_sub = pd.DataFrame(y)\n",
        "    D_sub = pd.DataFrame(D)\n",
        "\n",
        "    '''Cada linha é um sample, se ela tiver um peso maior, será selecionada.\n",
        "    Assim, se criará um novo conjunto de sub-amostragem.'''\n",
        "\n",
        "    # Retorna o % de tam das primeiras linhas mais acima na ordenação, pois são as com maiores pesos.\n",
        "    X_sub = X.head(tam)\n",
        "    y_sub = y.head(tam)\n",
        "    D_sub = D.head(tam)\n",
        "\n",
        "    return X_sub, y_sub, D_sub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TMJHMrgyRZy",
        "colab_type": "text"
      },
      "source": [
        "# AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIoK7hBxyRZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Adaboost (X, y, D):\n",
        "    \n",
        "    X_ = pd.DataFrame(X) \n",
        "    y_ = pd.DataFrame(y)\n",
        "    D_ = pd.DataFrame(D)\n",
        "    \n",
        "    # Modelo de aprendizagem fraca (árvore com profundidade 1 e, consequentemente, dois nodos filhos)\n",
        "    clf = DecisionTreeClassifier(max_depth = 1, max_leaf_nodes = 2, random_state = 0)\n",
        "\n",
        "    # Utilizando a função Dn\n",
        "    clf.fit(X_, y_, sample_weight = np.array(D_[1].tolist()))\n",
        "\n",
        "    # Hipótese Fn: X -> Y\n",
        "    hipotese = clf.predict(X_)\n",
        "    \n",
        "    #print(\"Hipotese: \\n\" + str(hipotese))\n",
        "\n",
        "    '''Duas variáveis controlam alguns laços abaixo, uma para o df (por causa dos seus índices) e outra para os vetores.''' \n",
        "\n",
        "    erros=[]\n",
        "    i=0\n",
        "    for index, row in y_.iterrows():\n",
        "        erros.append(row.item() != hipotese[i]) # Monta um vetor de erros onde a hipotese é diferente do label\n",
        "        i+=1\n",
        "\n",
        "    #print(\"Erros: \\n\" + str(erros))\n",
        "\n",
        "    # Erro da hipótese Fn\n",
        "    erro_hipotese=0\n",
        "    i=0\n",
        "    for index, row in D_.iterrows():\n",
        "        if erros[i]:\n",
        "            erro_hipotese += row.item() # Faz o somatório dos pesos das posições erradas\n",
        "        i+=1\n",
        "\n",
        "    #print(\"Erro da hipotese: \" + str(erro_hipotese))\n",
        "\n",
        "    # Faça Bn = 1/2 * log(1-En)/En\n",
        "    if erro_hipotese == 0: # Evita divisão por zero\n",
        "        beta = 0\n",
        "    else:\n",
        "        beta = 0.5 * np.log((1. - erro_hipotese) / erro_hipotese)\n",
        "\n",
        "    #print(\"Beta: \" + str(beta))\n",
        "\n",
        "    # Converte y de DataFrame para array para não dar problema no cálculo de D abaixo\n",
        "    y_array = y_.to_numpy() \n",
        "\n",
        "    # Atualiza a distribuição Dn\n",
        "    j=0\n",
        "    for i in D_.index:\n",
        "        D_.at[i,1] = D_.at[i,1] * np.exp(-beta * y_array[j] * hipotese[j])\n",
        "        j+=1\n",
        "\n",
        "    ''' Descomentar esse bloco para ver a acurácia na sub-amostragem\n",
        "\n",
        "    # Guarda no vetor resultado a hipótese final do modelo\n",
        "    resultado = []\n",
        "    for j in hipotese:\n",
        "      resultado.append(np.sign(j * beta))\n",
        "\n",
        "    # Soma 1 ao total de acertos cada vez que o resultado da hipótese for igual ao label\n",
        "    total=0\n",
        "    k=0\n",
        "    for index, row in y_.iterrows():\n",
        "      if resultado[k] == row.item():\n",
        "        total+=1\n",
        "      k+=1\n",
        "    \n",
        "    print(\"\\nResultado na sub-amostragem:\", total/len(y_)*100, \"%\")\n",
        "    '''\n",
        "    \n",
        "    return D_, beta, hipotese, clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8eYS4u7DQnp",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzNM5RG6yRZ7",
        "colab_type": "code",
        "outputId": "90a2f441-87e4-4008-a7f1-808f75eac96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "sonar_df = pd.DataFrame(data, index=np.arange(1, 209), columns=np.arange(1, 61))\n",
        "labels_df = pd.DataFrame(labels, index=np.arange(1, 209), columns=[1])\n",
        "\n",
        "# Inicialização: D1(i)=1/N para todo i \n",
        "pesos = len(labels_df)\n",
        "\n",
        "# Peso inicial = 1/N\n",
        "pesos = pd.DataFrame(np.ones(pesos) / pesos, index=np.arange(1, 209), columns=[1])\n",
        "\n",
        "pesos_lista = []\n",
        "betas_lista = []\n",
        "hipoteses_lista = []\n",
        "resultados_lista = []\n",
        "y_preds = []\n",
        "acuracia_modelos = []\n",
        "\n",
        "X = pd.DataFrame(sonar_df) \n",
        "y = pd.DataFrame(labels_df)\n",
        "D = pd.DataFrame(pesos)\n",
        "\n",
        "T = 20\n",
        "kf = KFold(n_splits = T)\n",
        "\n",
        "# A sub-amostragem é feita com 10% das linhas do dataset original\n",
        "sub_tam = int((y.size*10)/100)\n",
        "\n",
        "fold = 1\n",
        "\n",
        "for i in kf.split(X):\n",
        "\n",
        "    # 70% treino e 30% teste - a opção de shuffle fica desligada para não mexer na ordem dos pesos\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle=False)\n",
        "    D_train = D.reindex(X_train.index) # Cria um conjunto de pesos D do mesmo tamanho de X_train\n",
        "\n",
        "    X_sub, y_sub, D_sub = sub_amostragem(sub_tam, X_train, y_train, D_train)\n",
        "\n",
        "    D_sub, beta, hipotese, modelo = Adaboost(X_sub, y_sub, D_sub)\n",
        "    \n",
        "    # Atualiza os pesos (D) nas posições da sub-amostragem (D_sub)\n",
        "    D.update(D_sub)\n",
        "\n",
        "    # Normaliza os pesos\n",
        "    for j in D.index:\n",
        "        D.at[j,1] = D.at[j,1] / sum(D)\n",
        "    \n",
        "    D.sort_values(by=1, ascending=False, inplace=True) # Reordena os pesos colocando os mais altos em primeiro\n",
        "    X = X.reindex(D.index) # Reordena os dados conforme os índices dos pesos (D)\n",
        "    y = y.reindex(D.index) # Reordena os labels também\n",
        "    \n",
        "    y_pred = modelo.predict(X_test)\n",
        "\n",
        "    # Guarda no vetor resultado a hipótese final do modelo\n",
        "    resultado = []\n",
        "    for j in y_pred:\n",
        "      resultado.append(np.sign(j * beta))\n",
        "\n",
        "    # Soma 1 ao total de acertos cada vez que o resultado da hipótese for igual ao label\n",
        "    total=0\n",
        "    k=0\n",
        "    for index, row in y_test.iterrows():\n",
        "      if resultado[k] == row.item():\n",
        "        total+=1\n",
        "      k+=1\n",
        "    \n",
        "    betas_lista.append(beta)\n",
        "    hipoteses_lista.append(hipotese)\n",
        "    pesos_lista.append(D)\n",
        "    y_preds.append(y_pred)\n",
        "    resultados_lista.append(resultado)\n",
        "    acuracia_modelos.append(total/len(y_test)*100)\n",
        "    \n",
        "    #Hipótese Final\n",
        "    print(\"Acurácia do modelo\", fold, \":\", str(total/len(y_test)*100), \"%\")\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "#Hipótese Final\n",
        "print(\"\\nMédia das acurácias:\", mean(acuracia_modelos), \"%\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia do modelo 1 : 58.730158730158735 %\n",
            "Acurácia do modelo 2 : 66.66666666666666 %\n",
            "Acurácia do modelo 3 : 50.79365079365079 %\n",
            "Acurácia do modelo 4 : 80.95238095238095 %\n",
            "Acurácia do modelo 5 : 69.84126984126983 %\n",
            "Acurácia do modelo 6 : 57.14285714285714 %\n",
            "Acurácia do modelo 7 : 57.14285714285714 %\n",
            "Acurácia do modelo 8 : 57.14285714285714 %\n",
            "Acurácia do modelo 9 : 50.79365079365079 %\n",
            "Acurácia do modelo 10 : 46.03174603174603 %\n",
            "Acurácia do modelo 11 : 79.36507936507937 %\n",
            "Acurácia do modelo 12 : 68.25396825396825 %\n",
            "Acurácia do modelo 13 : 58.730158730158735 %\n",
            "Acurácia do modelo 14 : 77.77777777777779 %\n",
            "Acurácia do modelo 15 : 52.38095238095239 %\n",
            "Acurácia do modelo 16 : 61.904761904761905 %\n",
            "Acurácia do modelo 17 : 68.25396825396825 %\n",
            "Acurácia do modelo 18 : 46.03174603174603 %\n",
            "Acurácia do modelo 19 : 57.14285714285714 %\n",
            "Acurácia do modelo 20 : 63.49206349206349 %\n",
            "\n",
            "Média das acurácias: 61.42857142857143 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}